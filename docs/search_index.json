[["index.html", "Méthodes Quantitatives en Géographie Introduction", " Méthodes Quantitatives en Géographie Hugo Périlleux 2024-10-18 Introduction Ce manuel est le matériel des travaux pratiques pour le cours GEOG-F-404 Méthode quantitative en géographie à l’Université Libre de Bruxelles. Il contient 3 chapitres: Régression linéaires Analyse en composantes principales Classification "],["régressions.html", "Chapter 1 Régressions 1.1 Préparation 1.2 Analyse de régression 1.3 Analyse géographique", " Chapter 1 Régressions données utilisées: immoweb_louer.csv census_2011_logements.xls cantons_judiciaires_bxl_2018.gpkg 1.1 Préparation Pour manipuler les données on utilisera les packages suivants: library(tidyverse) library(readxl) library(sf) library(mapsf) library(modelr) On va utiliser les nouveaux packages suivants: library(jtools) library(huxtable) library(ggstats) library(performance) Pour réaliser cet exemple, on utilisera les données issues d’un scraping de Immoweb des logements à louer pour Bruxelles. Les adresses ont été géocodées avec Phacochr. loyers_data&lt;-read_delim(&quot;data/immoweb_louer.csv&quot;, delim = &quot;;&quot;) ## Rows: 17735 Columns: 9 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (4): type, PEB, nb_chambres, cd_sector ## dbl (4): surface, loyer, x_31370, y_31370 ## dttm (1): date_request ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 1.2 Analyse de régression 1.2.1 Visualiser et supprimer les outliers On peut analyser la relation entre la variable loyer et la variable surface, on peut simplement réaliser un graphique avec un geom_point: loyers_data %&gt;% ggplot( aes(surface, loyer)) + geom_point(alpha=0.5,cex=1) ## Warning: Removed 2066 rows containing missing values or values ## outside the scale range (`geom_point()`). Pour analyser la relation entre les deux variables, il peut être utile de supprimer les valeurs abérantes (outliers). Ceci peut se faire de façon simple en retirant 1% de chaque côté de la distribution en appliquant un filtre et en conservant les loyer par surface supérieur au quantile 0.01 et inférieur au quantile 0.99: loyers_data&lt;- loyers_data %&gt;% filter( loyer &gt; quantile(loyer, prob = 0.01, na.rm = TRUE), loyer &lt; quantile(loyer, prob = 0.99, na.rm = TRUE), surface &gt; quantile(surface, prob = 0.01, na.rm = TRUE), surface &lt; quantile(surface, prob = 0.99, na.rm = TRUE)) Pour supprimer les outliers, il est possible également d’utiliser de filtre plus “réfléchi” en analysant les données ou en utilisant des méthodes plus complexes : https://delladata.fr/comment-detecter-les-outliers-avec-r/ On peut alors revisualiser les données et on observe une relation beaucoup plus claire: loyers_data %&gt;% ggplot( aes(surface, loyer)) + geom_point(alpha=0.3,cex=0.5) On peut ajouter une droite de régression sur le graphique grâce à geom_smooth: loyers_data %&gt;% ggplot( aes(surface, loyer)) + geom_point(alpha=0.3,cex=0.5)+ geom_smooth(formula = y ~ x, method = &quot;lm&quot;) 1.2.2 Régression simple On peut réaliser une analyse de régression grâce à la fonction lm où Y = aX +b ce traduit par Y ~ X. Dans notre cas, on tente d’évaluer le loyer (Y) en fonction de la surface (X): model1&lt;-lm(loyer ~ surface, data=loyers_data) Pour afficher un résumé des résultats on utilise la fonction summary sur l’objet créer par la fonction lm: summary(model1) ## ## Call: ## lm(formula = loyer ~ surface, data = loyers_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4328.9 -210.7 -50.8 147.6 3883.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 385.3549 7.0466 54.69 &lt;2e-16 *** ## surface 11.1933 0.0557 200.97 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 435.8 on 15222 degrees of freedom ## Multiple R-squared: 0.7263, Adjusted R-squared: 0.7263 ## F-statistic: 4.039e+04 on 1 and 15222 DF, p-value: &lt; 2.2e-16 Notez que l’objet model est un objet list qui contient une série de choses qu’il est possible d’aller chercher grâce au $ : model1\\(coefficients les coefficients model1\\)residuals les résidus model1$fitted.values les valeurs prédites De même il est possible d’aller rechercher des éléments du résultat de summary appliqué sur le modèle: summary(model1)$r.squared le R² 1.2.3 Régression multiple On peut décider de rajouter des variables dans le modèle en modifiant la formule Y = X1 + X2 + X3 + … model2&lt;-lm(loyer ~ surface + type + PEB + nb_chambres, data=loyers_data) summary(model2) ## ## Call: ## lm(formula = loyer ~ surface + type + PEB + nb_chambres, data = loyers_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4185.1 -212.7 -38.6 154.0 2892.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 408.4328 27.3727 14.921 &lt; 2e-16 *** ## surface 11.1487 0.1127 98.950 &lt; 2e-16 *** ## typeHOUSE 21.0105 19.6294 1.070 0.284480 ## PEBB 102.9783 23.7831 4.330 1.50e-05 *** ## PEBC 11.3038 22.9773 0.492 0.622762 ## PEBD -69.1262 22.7818 -3.034 0.002417 ** ## PEBE -82.0685 23.5270 -3.488 0.000488 *** ## PEBF -118.1271 25.2917 -4.671 3.04e-06 *** ## PEBG -99.2732 24.5295 -4.047 5.22e-05 *** ## nb_chambres1 -12.0105 19.0896 -0.629 0.529254 ## nb_chambres2 4.8550 19.6849 0.247 0.805194 ## nb_chambres3 156.4296 23.2023 6.742 1.64e-11 *** ## nb_chambres4 79.7030 31.7783 2.508 0.012153 * ## nb_chambres5+ -135.7740 42.2787 -3.211 0.001325 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 410.4 on 11090 degrees of freedom ## (4120 observations effacées parce que manquantes) ## Multiple R-squared: 0.7633, Adjusted R-squared: 0.763 ## F-statistic: 2751 on 13 and 11090 DF, p-value: &lt; 2.2e-16 1.2.4 Régression polynomiale 1.2.4.1 Logarithme Plutôt que de modéliser loyer en fonction de la surface, on pourrait avoir envie de modéliser le loyer selon le logarithme de la surface. Alors on soit créer une nouvelle variable et réaliser la régression: loyers_data$log_surface &lt;- log(loyers_data$surface) model3&lt;- lm(loyer ~ log_surface + type + PEB + nb_chambres, data=loyers_data) summary(model3) ## ## Call: ## lm(formula = loyer ~ log_surface + type + PEB + nb_chambres, ## data = loyers_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3415.2 -263.3 -50.6 200.8 2883.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4067.02 67.47 -60.282 &lt; 2e-16 *** ## log_surface 1320.08 16.31 80.923 &lt; 2e-16 *** ## typeHOUSE 98.95 21.29 4.649 3.38e-06 *** ## PEBB 92.62 25.88 3.578 0.000347 *** ## PEBC 11.21 25.01 0.448 0.653853 ## PEBD -56.46 24.79 -2.277 0.022783 * ## PEBE -67.66 25.60 -2.643 0.008226 ** ## PEBF -105.00 27.52 -3.816 0.000137 *** ## PEBG -72.71 26.68 -2.725 0.006443 ** ## nb_chambres1 -281.90 21.42 -13.161 &lt; 2e-16 *** ## nb_chambres2 -458.09 24.36 -18.804 &lt; 2e-16 *** ## nb_chambres3 -249.66 29.24 -8.538 &lt; 2e-16 *** ## nb_chambres4 -137.41 37.64 -3.650 0.000263 *** ## nb_chambres5+ 130.42 46.17 2.825 0.004743 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 446.6 on 11090 degrees of freedom ## (4120 observations effacées parce que manquantes) ## Multiple R-squared: 0.7198, Adjusted R-squared: 0.7194 ## F-statistic: 2191 on 13 and 11090 DF, p-value: &lt; 2.2e-16 soit indiquer le logarithme directement dans la formule du lm model3&lt;- lm(loyer ~ log(surface) + type + PEB + nb_chambres, data=loyers_data) summary(model3) ## ## Call: ## lm(formula = loyer ~ log(surface) + type + PEB + nb_chambres, ## data = loyers_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3415.2 -263.3 -50.6 200.8 2883.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4067.02 67.47 -60.282 &lt; 2e-16 *** ## log(surface) 1320.08 16.31 80.923 &lt; 2e-16 *** ## typeHOUSE 98.95 21.29 4.649 3.38e-06 *** ## PEBB 92.62 25.88 3.578 0.000347 *** ## PEBC 11.21 25.01 0.448 0.653853 ## PEBD -56.46 24.79 -2.277 0.022783 * ## PEBE -67.66 25.60 -2.643 0.008226 ** ## PEBF -105.00 27.52 -3.816 0.000137 *** ## PEBG -72.71 26.68 -2.725 0.006443 ** ## nb_chambres1 -281.90 21.42 -13.161 &lt; 2e-16 *** ## nb_chambres2 -458.09 24.36 -18.804 &lt; 2e-16 *** ## nb_chambres3 -249.66 29.24 -8.538 &lt; 2e-16 *** ## nb_chambres4 -137.41 37.64 -3.650 0.000263 *** ## nb_chambres5+ 130.42 46.17 2.825 0.004743 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 446.6 on 11090 degrees of freedom ## (4120 observations effacées parce que manquantes) ## Multiple R-squared: 0.7198, Adjusted R-squared: 0.7194 ## F-statistic: 2191 on 13 and 11090 DF, p-value: &lt; 2.2e-16 On peut faire un graphique de la façon suivante: loyers_data %&gt;% ggplot( aes(surface, loyer)) + geom_point(alpha=0.3,cex=0.5)+ geom_smooth(formula = y ~ log(x), method = &quot;lm&quot;) 1.2.4.2 Division On peut vouloir modéliser le loyer par surface en fonction de la surface: loyers_data &lt;- loyers_data %&gt;% mutate(loyer_surface= loyer/surface, inv_surface= 1/surface) model4&lt;- lm(loyer_surface ~ inv_surface + type + PEB + nb_chambres, data=loyers_data) summary(model4) ## ## Call: ## lm(formula = loyer_surface ~ inv_surface + type + PEB + nb_chambres, ## data = loyers_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.2378 -2.1955 -0.4891 1.6736 31.3294 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.3490 0.3133 26.647 &lt; 2e-16 *** ## inv_surface 513.5564 9.6383 53.283 &lt; 2e-16 *** ## typeHOUSE 0.3212 0.1522 2.111 0.0348 * ## PEBB 0.8469 0.1857 4.561 5.14e-06 *** ## PEBC -0.0173 0.1793 -0.097 0.9231 ## PEBD -0.7036 0.1776 -3.961 7.51e-05 *** ## PEBE -0.9432 0.1835 -5.139 2.80e-07 *** ## PEBF -1.1262 0.1973 -5.708 1.17e-08 *** ## PEBG -1.3072 0.1915 -6.827 9.11e-12 *** ## nb_chambres1 0.9402 0.1631 5.765 8.38e-09 *** ## nb_chambres2 1.8815 0.1948 9.659 &lt; 2e-16 *** ## nb_chambres3 3.2194 0.2225 14.472 &lt; 2e-16 *** ## nb_chambres4 3.0069 0.2709 11.099 &lt; 2e-16 *** ## nb_chambres5+ 2.3830 0.3180 7.494 7.17e-14 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.204 on 11090 degrees of freedom ## (4120 observations effacées parce que manquantes) ## Multiple R-squared: 0.3567, Adjusted R-squared: 0.356 ## F-statistic: 473.1 on 13 and 11090 DF, p-value: &lt; 2.2e-16 Le graphique avec le I dans le formule: loyers_data %&gt;% ggplot( aes(surface, loyer_surface)) + geom_point(alpha=0.3,cex=0.5)+ geom_smooth(formula = y ~ I(1/x), method = &quot;lm&quot;) 1.2.4.3 Exposants On pourrait directement placer ces changements dans la formule mais alors il faut utiliser la fonction I() qui permet de réaliser ces opérations. La fonction I ( “come si” / “as is”) permet d’indiquer qu’il s’agit d’une formule et non pas une opération sur un vecteur et le ~ “est dépendant de”. model5&lt;- lm(loyer~ surface + I(surface^2) + type + PEB + nb_chambres, data=loyers_data) summary(model5) ## ## Call: ## lm(formula = loyer ~ surface + I(surface^2) + type + PEB + nb_chambres, ## data = loyers_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4072.8 -213.4 -37.2 159.6 2881.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.562e+02 2.943e+01 12.105 &lt; 2e-16 *** ## surface 1.259e+01 3.200e-01 39.332 &lt; 2e-16 *** ## I(surface^2) -3.933e-03 8.187e-04 -4.804 1.58e-06 *** ## typeHOUSE 2.305e+01 1.961e+01 1.175 0.239960 ## PEBB 9.977e+01 2.377e+01 4.197 2.72e-05 *** ## PEBC 6.240e+00 2.298e+01 0.272 0.785963 ## PEBD -7.414e+01 2.278e+01 -3.254 0.001141 ** ## PEBE -8.608e+01 2.352e+01 -3.660 0.000253 *** ## PEBF -1.216e+02 2.528e+01 -4.811 1.52e-06 *** ## PEBG -1.019e+02 2.451e+01 -4.156 3.27e-05 *** ## nb_chambres1 -3.119e+01 1.948e+01 -1.601 0.109455 ## nb_chambres2 -4.329e+01 2.207e+01 -1.961 0.049890 * ## nb_chambres3 9.303e+01 2.667e+01 3.488 0.000489 *** ## nb_chambres4 1.865e+01 3.420e+01 0.545 0.585428 ## nb_chambres5+ -1.391e+02 4.224e+01 -3.293 0.000993 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 410 on 11089 degrees of freedom ## (4120 observations effacées parce que manquantes) ## Multiple R-squared: 0.7638, Adjusted R-squared: 0.7635 ## F-statistic: 2561 on 14 and 11089 DF, p-value: &lt; 2.2e-16 le graphique, de nouveau il faut bien utiliser I dans la formule: loyers_data %&gt;% ggplot( aes(surface, loyer)) + geom_point(alpha=0.3,cex=0.5)+ geom_smooth(formula = y ~ I(x+x^2), method = &quot;lm&quot;) 1.2.5 Visualisation Il existe beaucoup de package pour faciliter la visualisation des résultats. # Par exemple pour visualiser rapidement le résultat de plusieurs modèle jtools permet de réaliser un tableau synthétique utile: library(jtools) export_summs(model1, model2, model3, model4, model5) Model 1Model 2Model 3Model 4Model 5 (Intercept)385.35 ***408.43 ***-4067.02 ***8.35 ***356.21 *** (7.05)&nbsp;&nbsp;&nbsp;(27.37)&nbsp;&nbsp;&nbsp;(67.47)&nbsp;&nbsp;&nbsp;(0.31)&nbsp;&nbsp;&nbsp;(29.43)&nbsp;&nbsp;&nbsp; surface11.19 ***11.15 ***&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.59 *** (0.06)&nbsp;&nbsp;&nbsp;(0.11)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.32)&nbsp;&nbsp;&nbsp; typeHOUSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;21.01&nbsp;&nbsp;&nbsp;&nbsp;98.95 ***0.32 *&nbsp;&nbsp;23.05&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(19.63)&nbsp;&nbsp;&nbsp;(21.29)&nbsp;&nbsp;&nbsp;(0.15)&nbsp;&nbsp;&nbsp;(19.61)&nbsp;&nbsp;&nbsp; PEBB&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;102.98 ***92.62 ***0.85 ***99.77 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(23.78)&nbsp;&nbsp;&nbsp;(25.88)&nbsp;&nbsp;&nbsp;(0.19)&nbsp;&nbsp;&nbsp;(23.77)&nbsp;&nbsp;&nbsp; PEBC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11.30&nbsp;&nbsp;&nbsp;&nbsp;11.21&nbsp;&nbsp;&nbsp;&nbsp;-0.02&nbsp;&nbsp;&nbsp;&nbsp;6.24&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(22.98)&nbsp;&nbsp;&nbsp;(25.01)&nbsp;&nbsp;&nbsp;(0.18)&nbsp;&nbsp;&nbsp;(22.98)&nbsp;&nbsp;&nbsp; PEBD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-69.13 **&nbsp;-56.46 *&nbsp;&nbsp;-0.70 ***-74.14 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(22.78)&nbsp;&nbsp;&nbsp;(24.79)&nbsp;&nbsp;&nbsp;(0.18)&nbsp;&nbsp;&nbsp;(22.78)&nbsp;&nbsp;&nbsp; PEBE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-82.07 ***-67.66 **&nbsp;-0.94 ***-86.08 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(23.53)&nbsp;&nbsp;&nbsp;(25.60)&nbsp;&nbsp;&nbsp;(0.18)&nbsp;&nbsp;&nbsp;(23.52)&nbsp;&nbsp;&nbsp; PEBF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-118.13 ***-105.00 ***-1.13 ***-121.61 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(25.29)&nbsp;&nbsp;&nbsp;(27.52)&nbsp;&nbsp;&nbsp;(0.20)&nbsp;&nbsp;&nbsp;(25.28)&nbsp;&nbsp;&nbsp; PEBG&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-99.27 ***-72.71 **&nbsp;-1.31 ***-101.86 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(24.53)&nbsp;&nbsp;&nbsp;(26.68)&nbsp;&nbsp;&nbsp;(0.19)&nbsp;&nbsp;&nbsp;(24.51)&nbsp;&nbsp;&nbsp; nb_chambres1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-12.01&nbsp;&nbsp;&nbsp;&nbsp;-281.90 ***0.94 ***-31.19&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(19.09)&nbsp;&nbsp;&nbsp;(21.42)&nbsp;&nbsp;&nbsp;(0.16)&nbsp;&nbsp;&nbsp;(19.48)&nbsp;&nbsp;&nbsp; nb_chambres2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.86&nbsp;&nbsp;&nbsp;&nbsp;-458.09 ***1.88 ***-43.29 *&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(19.68)&nbsp;&nbsp;&nbsp;(24.36)&nbsp;&nbsp;&nbsp;(0.19)&nbsp;&nbsp;&nbsp;(22.07)&nbsp;&nbsp;&nbsp; nb_chambres3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;156.43 ***-249.66 ***3.22 ***93.03 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(23.20)&nbsp;&nbsp;&nbsp;(29.24)&nbsp;&nbsp;&nbsp;(0.22)&nbsp;&nbsp;&nbsp;(26.67)&nbsp;&nbsp;&nbsp; nb_chambres4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;79.70 *&nbsp;&nbsp;-137.41 ***3.01 ***18.65&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(31.78)&nbsp;&nbsp;&nbsp;(37.64)&nbsp;&nbsp;&nbsp;(0.27)&nbsp;&nbsp;&nbsp;(34.20)&nbsp;&nbsp;&nbsp; nb_chambres5+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-135.77 **&nbsp;130.42 **&nbsp;2.38 ***-139.12 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(42.28)&nbsp;&nbsp;&nbsp;(46.17)&nbsp;&nbsp;&nbsp;(0.32)&nbsp;&nbsp;&nbsp;(42.24)&nbsp;&nbsp;&nbsp; log(surface)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1320.08 ***&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(16.31)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inv_surface&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;513.56 ***&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(9.64)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I(surface^2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.00 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N15224&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11104&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11104&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11104&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11104&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.73&nbsp;&nbsp;&nbsp;&nbsp;0.76&nbsp;&nbsp;&nbsp;&nbsp;0.72&nbsp;&nbsp;&nbsp;&nbsp;0.36&nbsp;&nbsp;&nbsp;&nbsp;0.76&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. On peut également exporter ce résultat en format pdf de la façon suivante: # export_summs(model1, model2,model3, model4,to.file = &quot;html&quot;, file.name = &quot;TP08/tableau_regression.html&quot;) On peut visualiser un forest plot de cette façon grâce au package ggstats: library(ggstats) ggcoef_model(model2) ggcoef_table(model2) voir ici pour plus de détails: &lt; https://cran.r-project.org/web/packages/ggstats/vignettes/ggcoef_model.htm &gt; 1.2.6 Réaliser les tests des hypothèses La réalisation de modèle de régression s’appuie sur une série d’hypothèses qu’il s’agit de vérifier dont: - la linéarité des résidus, - la constance de la variance des résidus, - la faible influence d’outliers, - la non colinéarité entre les variables explicatives, - la normalité des résidus. &lt; https://easystats.github.io/performance/index.html &gt; library(performance) #png(&quot;TP08/check_model.png&quot;, height= 1000, width=1000) check_model(model2) #dev.off() 1.3 Analyse géographique 1.3.1 Analyse géographique simple On peut réaliser une carte du loyer moyen par secteur statistique. On calculer par secteur statistique le nombre d’annonces et le loyer moyen: loyer_moyen&lt;-loyers_data%&gt;% group_by(cd_sector) %&gt;% summarise( n=n(), loy_moyen= mean(loyer, na.rm=T) ) On charge les secteurs statistiques secteurs_stats&lt;- st_read (&quot;data/sh_statbel_statistical_sectors_31370_20230101.gpkg&quot;) ## Reading layer `secteurs_stats2023&#39; from data source ## `C:\\Users\\hugop\\Nextcloud\\git\\manuel_geo_quanti\\data\\sh_statbel_statistical_sectors_31370_20230101.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 19795 features and 31 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 21991.63 ymin: 21162.16 xmax: 295167.1 ymax: 244027.2 ## Projected CRS: BD72 / Belgian Lambert 72 On fait une jointure entre le résultat du calcul précédent et les secteurs statistiques loyer_moyen_sec&lt;-secteurs_stats %&gt;% left_join(loyer_moyen, by=c(&quot;cd_sector&quot;)) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) On réalise la carte mf_map(x = loyer_moyen_sec,col = &quot;white&quot;, border = &quot;grey&quot;) mf_map(loyer_moyen_sec, var= c(&quot;n&quot;, &quot;loy_moyen&quot;), type=&quot;prop_choro&quot;, pal= &quot;Viridis&quot;, inches=0.14, nbreaks=5, add=T) ## 196 &#39;NA&#39; values are not plotted on the map. 1.3.2 Cartographier les résidus Les résidus de la régression se trouve dans l’objet model produit par la fonction lm. Néanmoins il s’agit d’un objet list où le valeur manquantes ont été supprimée. Le package modelr permet de faire facilement la jointure entre les données de base et les résidus et calculer une moyenne des résidus par secteurs statistiques: library(modelr) resid&lt;-loyers_data %&gt;% add_residuals( model2) %&gt;% group_by(cd_sector) %&gt;% summarise(moyenne_residus= mean(resid, na.rm=T), n=n()) On joint les résidus au fichier des secteurs statistiques resid_sec&lt;-secteurs_stats%&gt;% left_join(resid, by=c(&quot;cd_sector&quot;)) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) On fait une carte des résidus moyen par secteurs stats mf_map(x = resid_sec,col = &quot;white&quot;, border = &quot;grey&quot;) mf_map(resid_sec, var= c(&quot;n&quot;, &quot;moyenne_residus&quot;), type=&quot;prop_choro&quot;, pal= &quot;Viridis&quot;, inches=0.15, nbreaks=5, add=T) ## 196 &#39;NA&#39; values are not plotted on the map. On peut alors observer là où le loyer est en moyenne pus élevé ou moins élevé que prédit par le modèle. 1.3.3 Ajouter une variable géographique On peut ajouter une variable géographique comme l’indice de difficulté par secteur statistique de 2010 pour Bruxelles qui une bonne approximation de la division sociale de l’espace bruxellois: indice &lt;- read_delim(&quot;data/indice_synthetique.csv&quot;, delim= &quot;;&quot;) ## Rows: 7752 Columns: 2 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): Secteur statistique ## dbl (1): Indice synthétique de difficulté 2010 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. indice_sec&lt;-secteurs_stats%&gt;% left_join(indice, by=c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) mf_map(indice_sec, var=&quot;Indice synthétique de difficulté 2010&quot;, type=&quot;choro&quot;, pal= &quot;Viridis&quot;, nbreaks=5) On peut alors ajouter cet indice dans un troisième modèle. Pour réaliser ceci on doit d’abord joindre les données loyers_data&lt;-loyers_data %&gt;% left_join(indice, by=c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) model6&lt;-lm(loyer ~ surface + PEB+type+nb_chambres +`Indice synthétique de difficulté 2010`, data=loyers_data) export_summs(model1, model2, model6) Model 1Model 2Model 3 (Intercept)385.35 ***408.43 ***504.53 *** (7.05)&nbsp;&nbsp;&nbsp;(27.37)&nbsp;&nbsp;&nbsp;(36.16)&nbsp;&nbsp;&nbsp; surface11.19 ***11.15 ***9.58 *** (0.06)&nbsp;&nbsp;&nbsp;(0.11)&nbsp;&nbsp;&nbsp;(0.16)&nbsp;&nbsp;&nbsp; typeHOUSE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;21.01&nbsp;&nbsp;&nbsp;&nbsp;75.26 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(19.63)&nbsp;&nbsp;&nbsp;(25.58)&nbsp;&nbsp;&nbsp; PEBB&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;102.98 ***90.67 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(23.78)&nbsp;&nbsp;&nbsp;(30.53)&nbsp;&nbsp;&nbsp; PEBC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11.30&nbsp;&nbsp;&nbsp;&nbsp;-30.36&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(22.98)&nbsp;&nbsp;&nbsp;(29.94)&nbsp;&nbsp;&nbsp; PEBD&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-69.13 **&nbsp;-79.09 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(22.78)&nbsp;&nbsp;&nbsp;(29.90)&nbsp;&nbsp;&nbsp; PEBE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-82.07 ***-111.85 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(23.53)&nbsp;&nbsp;&nbsp;(30.57)&nbsp;&nbsp;&nbsp; PEBF&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-118.13 ***-119.83 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(25.29)&nbsp;&nbsp;&nbsp;(32.58)&nbsp;&nbsp;&nbsp; PEBG&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-99.27 ***-138.68 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(24.53)&nbsp;&nbsp;&nbsp;(31.69)&nbsp;&nbsp;&nbsp; nb_chambres1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-12.01&nbsp;&nbsp;&nbsp;&nbsp;7.97&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(19.09)&nbsp;&nbsp;&nbsp;(23.97)&nbsp;&nbsp;&nbsp; nb_chambres2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.86&nbsp;&nbsp;&nbsp;&nbsp;73.96 **&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(19.68)&nbsp;&nbsp;&nbsp;(24.99)&nbsp;&nbsp;&nbsp; nb_chambres3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;156.43 ***250.02 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(23.20)&nbsp;&nbsp;&nbsp;(29.66)&nbsp;&nbsp;&nbsp; nb_chambres4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;79.70 *&nbsp;&nbsp;283.29 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(31.78)&nbsp;&nbsp;&nbsp;(40.85)&nbsp;&nbsp;&nbsp; nb_chambres5+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-135.77 **&nbsp;75.90&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(42.28)&nbsp;&nbsp;&nbsp;(54.97)&nbsp;&nbsp;&nbsp; `Indice synthétique de difficulté 2010`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.95&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6.16)&nbsp;&nbsp;&nbsp; N15224&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11104&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5981&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.73&nbsp;&nbsp;&nbsp;&nbsp;0.76&nbsp;&nbsp;&nbsp;&nbsp;0.73&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Notez que ici le modèle semble se détériorer puisque le R2 diminue. Ceci peut s’expliquer par le fait que le modèle est réaliser sur un nombre plus petit de variables. 1.3.4 Analyse de corrélation spatiale On peut également réaliser des régressions sur des entités spatiales. Ici on prend les données expulisions et on va analyser la correlation avec la division sociale de l’espace et les cantons pour mesurer un possible “effet juge”. On importe les données: expulsions &lt;- read_delim(&quot;data/expulsions_2018_secteursstat.csv&quot;, delim= &quot;;&quot;) ## Rows: 698 Columns: 7 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): ID_SS_bis ## dbl (6): ID_SS, exp_ais, exp_individu, exp_public, exp_so... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. indice &lt;- read_delim(&quot;data/indice_synthetique.csv&quot;, delim= &quot;;&quot;) ## Rows: 7752 Columns: 2 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): Secteur statistique ## dbl (1): Indice synthétique de difficulté 2010 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. logements &lt;- read_excel(&quot;data/census_2011_logements.xls&quot;) secteurs_stats&lt;- st_read (&quot;data/sh_statbel_statistical_sectors_31370_20230101.gpkg&quot;) ## Reading layer `secteurs_stats2023&#39; from data source ## `C:\\Users\\hugop\\Nextcloud\\git\\manuel_geo_quanti\\data\\sh_statbel_statistical_sectors_31370_20230101.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 19795 features and 31 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 21991.63 ymin: 21162.16 xmax: 295167.1 ymax: 244027.2 ## Projected CRS: BD72 / Belgian Lambert 72 cantons&lt;- st_read (&quot;data/cantons_judiciaires_bxl_2018.gpkg&quot;) %&gt;% st_zm() ## Reading layer `cantons_bxl_2018&#39; from data source ## `C:\\Users\\hugop\\Nextcloud\\git\\manuel_geo_quanti\\data\\cantons_judiciaires_bxl_2018.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 20 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XYZ ## Bounding box: xmin: 141192.7 ymin: 161464.7 xmax: 158003.9 ymax: 178175.9 ## z_range: zmin: 0 zmax: 0 ## Projected CRS: BD72 / Belgian Lambert 72 On réalise la jointure spatiale entre les canton et les centroïdes des secteurs statistiques: canton_secteurs&lt;-cantons %&gt;% st_join(st_point_on_surface(secteurs_stats), join= st_intersects) %&gt;% as.data.frame() %&gt;% select(CANTON, cd_sector) ## Warning: st_point_on_surface assumes attributes are constant ## over geometries 1.3.4.1 Analyse visuelle On peut réaliser une cartographie des trois jeux de données 1.3.4.1.1 Les expulsions secteurs_stats_epulsions&lt;-secteurs_stats %&gt;% left_join(expulsions, by=c(&quot;cd_sector&quot;=&quot;ID_SS_bis&quot;)) %&gt;% left_join(logements, by= c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) %&gt;% mutate(tx_expulsion=100*exp_total /`Logements loués`) mf_map(x = secteurs_stats_epulsions,col = &quot;white&quot;, border = &quot;grey&quot;) mf_map(secteurs_stats_epulsions, var=c(&quot;exp_total&quot;, &quot;tx_expulsion&quot;), type=&quot;prop_choro&quot;, pal= &quot;Viridis&quot;, inches=0.11, nbreaks=5, add=T) ## 26 &#39;NA&#39; values are not plotted on the map. ## 140 &#39;0&#39; values are not plotted on the map. 1.3.4.1.2 Les cantons: secteurs_stats_cantons&lt;-secteurs_stats %&gt;% left_join(canton_secteurs, by=&quot;cd_sector&quot;) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) mf_map(secteurs_stats_cantons, var= &quot;CANTON&quot;, type=&quot;typo&quot;) 1.3.4.1.3 L’indice synthétique de difficulté: secteurs_stats_indice&lt;-secteurs_stats %&gt;% left_join(indice, by= c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) mf_map(secteurs_stats_indice, var= &quot;Indice synthétique de difficulté 2010&quot;, type=&quot;choro&quot;, pal= &quot;Viridis&quot;) 1.3.4.2 Régression On réalise les jointures, on calcul un taux d’expulsions et on ne garde que les secteurs statistiques bruxellois qui ont plus de 200 logements loués secteurs_stats_expulsions&lt;-secteurs_stats %&gt;% left_join(expulsions, by=c(&quot;cd_sector&quot;=&quot;ID_SS_bis&quot;)) %&gt;% left_join(logements, by= c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) %&gt;% left_join(indice, by= c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) %&gt;% left_join(canton_secteurs,by= c(&quot;cd_sector&quot;)) %&gt;% mutate(tx_expulsion=100*exp_total /`Logements loués`) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) %&gt;% filter (`Logements loués`&gt;200) On peut analyser le lien entre indice synthétique et le taux d’expulsion: secteurs_stats_expulsions%&gt;% ggplot( aes(`Indice synthétique de difficulté 2010`, tx_expulsion)) + geom_point(alpha=0.3,cex=0.5)+ geom_smooth(formula = y ~ x, method = &quot;lm&quot;) Et réaliser une régression model1&lt;-lm(tx_expulsion~ `Indice synthétique de difficulté 2010`,data= secteurs_stats_expulsions ) summary(model1) ## ## Call: ## lm(formula = tx_expulsion ~ `Indice synthétique de difficulté 2010`, ## data = secteurs_stats_expulsions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.49704 -0.43992 -0.08241 0.37774 2.96598 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 1.15698 0.04469 ## `Indice synthétique de difficulté 2010` 0.15447 0.03208 ## t value Pr(&gt;|t|) ## (Intercept) 25.887 &lt; 2e-16 *** ## `Indice synthétique de difficulté 2010` 4.815 2.02e-06 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6788 on 450 degrees of freedom ## Multiple R-squared: 0.04899, Adjusted R-squared: 0.04688 ## F-statistic: 23.18 on 1 and 450 DF, p-value: 2.018e-06 On peut à nouveau cartographier les résidus secteurs_stats_expulsions1&lt;-secteurs_stats_expulsions %&gt;% add_residuals( model1) mf_map(x = filter(secteurs_stats, tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;), col = &quot;white&quot;, border = &quot;grey&quot;) mf_map(secteurs_stats_expulsions1, var=c(&quot;exp_total&quot;, &quot;resid&quot;), type=&quot;prop_choro&quot;, pal= &quot;Viridis&quot;, inches=0.11, nbreaks=5, add=T) ## 11 &#39;0&#39; values are not plotted on the map. On peut pondéré la régression par le nombre de logements loués. Dans le cas où on a des entités de tailles très différentes cela peut avoir du sens model2&lt;-lm(tx_expulsion~ `Indice synthétique de difficulté 2010`, weights = `Logements loués`,data= secteurs_stats_expulsions ) summary(model2) ## ## Call: ## lm(formula = tx_expulsion ~ `Indice synthétique de difficulté 2010`, ## data = secteurs_stats_expulsions, weights = `Logements loués`) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -110.573 -9.817 -1.243 9.429 63.777 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 1.17420 0.04720 ## `Indice synthétique de difficulté 2010` 0.10944 0.03095 ## t value Pr(&gt;|t|) ## (Intercept) 24.879 &lt; 2e-16 *** ## `Indice synthétique de difficulté 2010` 3.536 0.000448 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.14 on 450 degrees of freedom ## Multiple R-squared: 0.02704, Adjusted R-squared: 0.02488 ## F-statistic: 12.5 on 1 and 450 DF, p-value: 0.000448 secteurs_stats_expulsions%&gt;% ggplot( aes(x=`Indice synthétique de difficulté 2010`,y= tx_expulsion)) + geom_point(alpha=0.3, aes( size= `Logements loués` ) )+ geom_smooth(formula = y ~ x, method = &quot;lm&quot;, mapping = aes(weight = `Logements loués`)) On peut ajouter la variable canton pour mesurer “l’effet juge” model3&lt;-lm(tx_expulsion~ `Indice synthétique de difficulté 2010`+ CANTON,data= secteurs_stats_expulsions ) library(ggstats) ggcoef_table(model3) "],["analyse-en-composantes-principales.html", "Chapter 2 Analyse en Composantes Principales 2.1 Objectif: 2.2 Données: 2.3 Importation des données 2.4 Recodage 2.5 Première visualisation des données 2.6 Réaliser l’ACP 2.7 Visualiser les résultats 2.8 Ajouter des variables supplémentaires 2.9 Aller plus loin 2.10 Ressources utilisées:", " Chapter 2 Analyse en Composantes Principales 2.1 Objectif: L’objectif de ce TP est de réaliser une analyse en composante principale sur base de données électorales. 2.2 Données: données des élections présidentielles 2022 par communes : pres2022comm.csv (https://unehistoireduconflitpolitique.fr/telecharger.html) Ce fichier contient les résultats des élections présidentielles de 2022 en format csv. Ces fichiers ont été générés à partir des résultats électoraux disponibles sur le site du Ministère de l’Intérieur et sur data.gouv.fr. J. Cagé et T. Piketty (2023) : Une histoire du conflit politique. Élections et inégalités sociales en France, 1789-2022. Le Seuil. communes: communes-20220101.shp (1) arrondissement municipaux: arrondissements_municipaux-20180711.shp (2) table commune - région : commune2021.csv (3) revenus par commune : FILO2021_DEC_COM.xlsx (4) population par commune: population_insee_2021.xlsx (5) INSEE: https://www.data.gouv.fr/fr/datasets/decoupage-administratif-communal-francais-issu-d-openstreetmap/ INSEE: https://www.data.gouv.fr/fr/datasets/decoupage-administratif-communal-francais-issu-d-openstreetmap/ INSEE: https://www.insee.fr/fr/information/2560452 INSEE: https://www.insee.fr/fr/statistiques/7756855?sommaire=7756859 INSEE : https://www.insee.fr/fr/statistiques/7739582?sommaire=7728826 Pour ce TP nous utiliserons les packages suivants: De façon maintenant habituelle: library(tidyverse) library(sf) library(mapsf) library(readxl) library(remotes) Et deux packages développés pour réaliser des ACP et visualiser les résultats: #install_version(&quot;estimability&quot;, &quot;1.4.1&quot;) #install.packages(&quot;FactoMineR&quot;) #install.packages(&quot;factoextra&quot;) library(FactoMineR) library(factoextra) Les objectifs pour réaliser une ACP sont généralement : Débroussailler un large set de données avec plein de variables Réaliser un indicateur synthétique Préparer à une classification Dans le cas ici, on va plutôt réaliser une ACP suivant le premier objectif sur les votes exprimés aux élections en Île de France aux élections présidentielles de 2022. L’objectif sera de voir on retrouve des structures dans la géographie de cette élection. 2.3 Importation des données 2.3.1 Élections 2022 Importation des données: data&lt;-read_delim(&quot;data/France/pres2022comm.csv&quot;, delim=&quot;,&quot;) %&gt;% select(1:19) %&gt;% filter(codecommune!=&quot;75056&quot;) # %&gt;% # On enlève la communes de Paris ## Rows: 34867 Columns: 97 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): dep, nomdep, codecommune, nomcommune ## dbl (93): inscrits, votants, exprimes, voixARTHAUD, voixP... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. On prend une table pour ne sélectionner que l’Île de France: communes_table&lt;-read_delim(&quot;data/France/commune2021.csv&quot;) ## Rows: 37742 Columns: 12 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (11): TYPECOM, COM, REG, DEP, CTCD, ARR, NCC, NCCENR,... ## dbl (1): TNCC ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. data&lt;-data %&gt;% left_join(communes_table, by=c(&quot;codecommune&quot;=&quot;COM&quot;))%&gt;% filter(REG==11) %&gt;% select(-c(20:30)) 2.3.2 Communes communes&lt;- st_read(&quot;data/France/communes_arrond_ile_de_france.gpkg&quot;) %&gt;% filter(insee!=&quot;75056&quot;) ## Reading layer `communes_arrond_ile_de_france&#39; from data source `C:\\Users\\hugop\\Nextcloud\\git\\manuel_geo_quanti\\data\\France\\communes_arrond_ile_de_france.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 1287 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 1.446244 ymin: 48.12015 xmax: 3.559221 ymax: 49.24143 ## Geodetic CRS: WGS 84 # On importe les arrondissements de Paris, Lyon et Marseille arrond_lyon_mars_paris&lt;- st_read(&quot;data/France/arrondissements_municipaux-20180711-shp/arrondissements_municipaux-20180711.shp&quot;)%&gt;% select(-surf_km2) %&gt;% rename(geom=geometry) ## Reading layer `arrondissements_municipaux-20180711&#39; from data source `C:\\Users\\hugop\\Nextcloud\\git\\manuel_geo_quanti\\data\\France\\arrondissements_municipaux-20180711-shp\\arrondissements_municipaux-20180711.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 45 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 2.224122 ymin: 43.19714 xmax: 5.532476 ymax: 48.90216 ## Geodetic CRS: WGS 84 communes&lt;-bind_rows(communes,arrond_lyon_mars_paris) # On joint communes et arrondissements rm(arrond_lyon_mars_paris) On ne garde que les communes de l’Ile de France: communes&lt;-communes %&gt;% left_join(communes_table, by=c(&quot;insee&quot;=&quot;COM&quot;))%&gt;% filter(REG==11) %&gt;% select(-c(5:15)) ## Warning in sf_column %in% names(g): Detected an unexpected many-to-many relationship between `x` ## and `y`. ## ℹ Row 17 of `x` matches multiple rows in `y`. ## ℹ Row 31675 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set ## `relationship = &quot;many-to-many&quot;` to silence this warning. On calcul la densité par communes: communes&lt;-communes %&gt;% mutate(surface=as.numeric(st_area(geom))/10000) # On calcul le superficie de chaque communes On créé un objet département pour la cartographie: departements_Paris&lt;-communes %&gt;% left_join(communes_table, by=c(&quot;insee&quot;=&quot;COM&quot;)) %&gt;% filter(REG==11) %&gt;% group_by(DEP,REG) %&gt;% summarise(geom=st_union(geom)) ## Warning in sf_column %in% names(g): Detected an unexpected many-to-many relationship between `x` ## and `y`. ## ℹ Row 17 of `x` matches multiple rows in `y`. ## ℹ Row 31675 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set ## `relationship = &quot;many-to-many&quot;` to silence this warning. ## `summarise()` has grouped output by &#39;DEP&#39;. You can override ## using the `.groups` argument. rm(communes_table) 2.4 Recodage Pour réaliser l’ACP il faut d’abord recoder les variables. On utilise généralement des variables en proportion: data&lt;-data %&gt;% mutate(ARTHAUD=voixARTHAUD/exprimes, POUTOU=voixPOUTOU/exprimes, ROUSSEL=voixROUSSEL/exprimes, MELENCHON=voixMELENCHON/exprimes, JADOT=voixJADOT/exprimes, HIDALGO=voixHIDALGO/exprimes, LASSALLE=voixLASSALLE/exprimes, MACRON=voixMACRON/exprimes, PECRESSE=voixPECRESSE/exprimes, ZEMMOUR=voixZEMMOUR/exprimes, DUPONTAIGNAN=voixDUPONTAIGNAN/exprimes, MLEPEN=voixMLEPEN/exprimes) # De façon raccourcie, il est possible de réaliser de la façon suivante: # - en utilisant des pivot (tidyr) et un group by # data&lt;-data %&gt;% # pivot_longer(cols= 8:19,names_to =&quot;candidat&quot;, values_to = &quot;voix&quot; ) %&gt;% # mutate(voix=voix/exprimes) %&gt;% # pivot_wider(values_from = &quot;voix&quot;, names_from = &quot;candidat&quot;) # - de façon encore plus synthétique, en utilisant un across: # data&lt;-data %&gt;% # mutate(across(8:19, ~ . /exprimes)) 2.5 Première visualisation des données communes_voix&lt;-communes %&gt;% left_join(data, by=c(&quot;insee&quot;=&quot;codecommune&quot;)) 2.5.1 Une carte par variable On peut réaliser une carte pour chaque candidat: for (i in 12:23) { # mf_export(x = communes_voix , # filename =paste0(&quot;TP09/cartes_candidats/carte_&quot;,names(communes_voix)[i],&quot;.png&quot; ), # width = 900) mf_map(x = communes_voix, col = NA, border = &quot;gray25&quot;, lwd = 0.1) mf_map(x = departements_Paris, col = NA, border = &quot;gray25&quot;, lwd = 1, add=T) mf_map(communes_voix , var= c(names(communes_voix)[i], names(communes_voix)[i+12]), #val_max=max(unlist(communes_voix[,11:22])), type=&quot;prop_choro&quot;, pal=&quot;Viridis&quot;, inches=0.1, add=T) #dev.off() } ## 184 &#39;0&#39; values are not plotted on the map. ## 101 &#39;0&#39; values are not plotted on the map. ## 42 &#39;0&#39; values are not plotted on the map. ## 8 &#39;0&#39; values are not plotted on the map. ## 97 &#39;0&#39; values are not plotted on the map. ## 18 &#39;0&#39; values are not plotted on the map. ## 19 &#39;0&#39; values are not plotted on the map. rm(i) 2.5.2 Matrice de corrélation On peut réaliser une matrice de corrélation: cor.mat &lt;- round(cor(data[,20:31], use=&quot;complete.obs&quot;),2) # round(,2) permet d&#39;arrondir à deux décimale et use=&quot;complete.obs&quot; permet d&#39;exclure les NA cor.mat ## ARTHAUD POUTOU ROUSSEL MELENCHON JADOT HIDALGO ## ARTHAUD 1.00 0.09 0.05 0.05 -0.16 -0.04 ## POUTOU 0.09 1.00 0.14 0.00 -0.06 -0.03 ## ROUSSEL 0.05 0.14 1.00 0.17 -0.01 0.12 ## MELENCHON 0.05 0.00 0.17 1.00 -0.15 0.11 ## JADOT -0.16 -0.06 -0.01 -0.15 1.00 0.33 ## HIDALGO -0.04 -0.03 0.12 0.11 0.33 1.00 ## LASSALLE 0.08 0.10 0.01 -0.35 -0.06 -0.07 ## MACRON -0.29 -0.24 -0.22 -0.29 0.58 0.20 ## PECRESSE -0.13 -0.07 -0.26 -0.54 0.06 -0.11 ## ZEMMOUR -0.07 -0.13 -0.27 -0.53 0.10 -0.14 ## DUPONTAIGNAN 0.05 0.06 0.09 -0.29 -0.07 -0.11 ## MLEPEN 0.20 0.17 0.03 -0.40 -0.54 -0.31 ## LASSALLE MACRON PECRESSE ZEMMOUR DUPONTAIGNAN ## ARTHAUD 0.08 -0.29 -0.13 -0.07 0.05 ## POUTOU 0.10 -0.24 -0.07 -0.13 0.06 ## ROUSSEL 0.01 -0.22 -0.26 -0.27 0.09 ## MELENCHON -0.35 -0.29 -0.54 -0.53 -0.29 ## JADOT -0.06 0.58 0.06 0.10 -0.07 ## HIDALGO -0.07 0.20 -0.11 -0.14 -0.11 ## LASSALLE 1.00 -0.17 0.08 0.14 0.16 ## MACRON -0.17 1.00 0.25 0.25 -0.16 ## PECRESSE 0.08 0.25 1.00 0.32 -0.02 ## ZEMMOUR 0.14 0.25 0.32 1.00 -0.01 ## DUPONTAIGNAN 0.16 -0.16 -0.02 -0.01 1.00 ## MLEPEN 0.31 -0.68 -0.03 -0.03 0.29 ## MLEPEN ## ARTHAUD 0.20 ## POUTOU 0.17 ## ROUSSEL 0.03 ## MELENCHON -0.40 ## JADOT -0.54 ## HIDALGO -0.31 ## LASSALLE 0.31 ## MACRON -0.68 ## PECRESSE -0.03 ## ZEMMOUR -0.03 ## DUPONTAIGNAN 0.29 ## MLEPEN 1.00 Le package corrplot permet de visualiser cette matrice de corrélation de façon plus intuitive: library(&quot;corrplot&quot;) corrplot(cor.mat, tl.col=&quot;black&quot;) On peut exporter le résultat avec la fonction png de la façon suivante: #png(file=&quot;TP09/matrice_correlation.png&quot;, width = 1000, height = 1000) corrplot(cor.mat, tl.col=&quot;black&quot;) #dev.off() On voit entre autres des corrélations : positives entre : le vote pour Hidalgo, Jadot et Macron, le vote Pécresse, Zemmour et Macron négative entre: le vote Marine Le Pen et le vote Jadot-Macron le vote Mélanchon et le vote Pecresse-Zemmour Très peu de corrélation entre les vote Zemmour et Marine Lepene. rm(cor.mat) 2.6 Réaliser l’ACP 2.6.1 Une ACP non pondérée pca&lt;-PCA(data[,20:31], graph=T, ncp = NULL) L’objet pca est une liste dans lequel se trouve tous les résultats de l’ACP. On peut y accéder directement ou utiliser des fonctions préfaites pour visualiser les résultats: # name description # 1 &quot;$eig&quot; &quot;eigenvalues&quot; # 2 &quot;$var&quot; &quot;results for the variables&quot; # 3 &quot;$var$coord&quot; &quot;coord. for the variables&quot; # 4 &quot;$var$cor&quot; &quot;correlations variables - dimensions&quot; # 5 &quot;$var$cos2&quot; &quot;cos2 for the variables&quot; # 6 &quot;$var$contrib&quot; &quot;contributions of the variables&quot; # 7 &quot;$ind&quot; &quot;results for the individuals&quot; # 8 &quot;$ind$coord&quot; &quot;coord. for the individuals&quot; # 9 &quot;$ind$cos2&quot; &quot;cos2 for the individuals&quot; # 10 &quot;$ind$contrib&quot; &quot;contributions of the individuals&quot; # 11 &quot;$call&quot; &quot;summary statistics&quot; # 12 &quot;$call$centre&quot; &quot;mean of the variables&quot; # 13 &quot;$call$ecart.type&quot; &quot;standard error of the variables&quot; # 14 &quot;$call$row.w&quot; &quot;weights for the individuals&quot; # 15 &quot;$call$col.w&quot; &quot;weights for the variables&quot; 2.6.2 Pondérer le calcul de l’ACP Dans le cas précédent chacuns des individus à le même poid et chaque variable également. On peut décider de modifier les poids. Pourtant, il existe de grandes différences entre communes: # mf_export(x = communes_voix , # filename =paste0(&quot;TP09/cartes_votes_exprimes.png&quot; ), # width = 900) mf_map(x = communes_voix, col = NA, border = &quot;gray25&quot;, lwd = 0.1) mf_map(x = departements_Paris, col = NA, border = &quot;gray25&quot;, lwd = 1, add=T) mf_map(communes_voix , var= &quot;exprimes&quot;, type=&quot;prop&quot;, inches=0.1, add=T) #dev.off() rm(communes_voix) Si on souhaite pondérer les entités on peut utiliser “row.w =”: pca&lt;-PCA(data[,20:31], row.w =data$exprimes,ncp = NULL) Pour pondérer les variables on peut utiliser col.w Attention: pour pondérer, il faut que tous les poids soient &gt;0. Dans le cas présent ça ne pose pas de problème. 2.7 Visualiser les résultats 2.7.1 Cercles de corrélation Ce résultat sont stocker dans l’objet pca: pca$var$coord ## Dim.1 Dim.2 Dim.3 ## ARTHAUD 0.75783970 0.193700662 0.053886556 ## POUTOU 0.70415312 0.004765184 0.418058777 ## ROUSSEL 0.66687033 -0.124984377 0.299327347 ## MELENCHON 0.74370403 -0.538863968 -0.340415783 ## JADOT -0.45229731 -0.533488252 0.663311687 ## HIDALGO -0.10246245 -0.687312206 0.620349032 ## LASSALLE 0.07686699 0.764044366 0.449976355 ## MACRON -0.94011924 -0.048504652 0.166318959 ## PECRESSE -0.88585300 0.321419606 -0.009943942 ## ZEMMOUR -0.79129943 0.372205374 -0.100007056 ## DUPONTAIGNAN 0.19419953 0.681063286 0.357058840 ## MLEPEN 0.45088539 0.809295495 0.102917819 ## Dim.4 Dim.5 Dim.6 ## ARTHAUD -0.33202051 0.22476966 -0.270976655 ## POUTOU -0.07460159 0.23692366 0.499863557 ## ROUSSEL 0.62640195 0.14923271 -0.131302409 ## MELENCHON -0.02007377 -0.09595007 0.007735581 ## JADOT -0.03924331 -0.01890221 -0.069747306 ## HIDALGO -0.17878940 -0.04634065 -0.040984394 ## LASSALLE -0.03134172 0.14026444 -0.191952534 ## MACRON 0.02380460 0.02352660 -0.043682377 ## PECRESSE 0.04293183 0.14109764 0.017812438 ## ZEMMOUR 0.06375980 0.19413646 0.158100636 ## DUPONTAIGNAN 0.04326915 -0.55375208 0.082126654 ## MLEPEN -0.06620236 0.04182531 -0.004296971 ## Dim.7 Dim.8 Dim.9 ## ARTHAUD 0.38461093 -0.05343325 -0.0045179775 ## POUTOU 0.02458871 -0.12931404 -0.0007009491 ## ROUSSEL 0.12543543 0.02976306 -0.0269964797 ## MELENCHON -0.03917968 -0.02553644 0.1644913917 ## JADOT 0.01101491 -0.04680005 -0.0019169799 ## HIDALGO -0.01196939 0.24982650 0.0054539159 ## LASSALLE -0.29553028 -0.08181916 0.2339225539 ## MACRON 0.04938810 -0.18414230 -0.1216818436 ## PECRESSE 0.11278338 -0.11425240 -0.0094676328 ## ZEMMOUR 0.20993077 0.27169444 0.1793777244 ## DUPONTAIGNAN 0.22599626 -0.03151295 0.0577028315 ## MLEPEN -0.15138365 0.19561196 -0.2456237780 ## Dim.10 Dim.11 Dim.12 ## ARTHAUD -0.008370156 -0.0140271624 3.282270e-10 ## POUTOU 0.015475982 -0.0156209607 3.891439e-10 ## ROUSSEL 0.028203308 -0.0056358853 1.336890e-09 ## MELENCHON 0.010914950 0.0401760796 2.360485e-08 ## JADOT -0.225853682 0.1036719950 3.831603e-09 ## HIDALGO 0.183340122 -0.0166654983 1.030643e-09 ## LASSALLE 0.025984130 -0.0303200827 9.593980e-10 ## MACRON 0.031798367 -0.1759761971 1.521580e-08 ## PECRESSE 0.152069481 0.2019841257 5.393004e-09 ## ZEMMOUR -0.079291052 -0.0469535055 5.553946e-09 ## DUPONTAIGNAN 0.017115750 0.0000869299 1.563518e-09 ## MLEPEN -0.040599884 0.0428961375 1.323804e-08 Les cercle de corrélation permettent de visualiser entre les variables et les dimensions produites par l’ACP. cercle_1_2&lt;-fviz_pca_var(pca, axes = c(1, 2))+ theme_minimal() cercle_1_2 #ggsave(cercle_1_2,filename=&quot;TP09/cercle_correlation_1_2.png&quot;, width=7, height=7, bg=&quot;white&quot;) #rm(cercle_1_2) On peut décider de visualiser d’autres axes en modifiant le paramètre axes: cercle_1_3&lt;-fviz_pca_var(pca, axes = c(1, 3))+ theme_minimal() cercle_1_3 #ggsave(cercle_1_3,filename=&quot;TP09/cercle_correlation_1_3.png&quot;, width=7, height=7, bg=&quot;white&quot;) #rm(cercle_1_3) Pour faciliter la lecture, on peut inverser un axe en multipliant par -1 (dans notre cas, il est plus intuitif d’avoir la gauche politique à gauche et la droite à droite): pca$var$coord[,1]&lt;- -1*pca$var$coord[,1] cercle_1_2&lt;-fviz_pca_var(pca, axes = c(1, 2) )+ theme_minimal() cercle_1_2 #ggsave(cercle_1_2,filename=&quot;TP09/cercle_correlation_1_2.png&quot;, width=7, height=7, bg=&quot;white&quot;) Il faut alors aussi le faire sur les coordonnées des individus: pca$ind$coord[,1]&lt;- -1* pca$ind$coord[,1] On peut décider de ne pas afficher toutes les variables (si il y en a trop ça devient illisible). Ici, on décide d’afficher surtout (en modifiant la tranparance) celles qui ont le plus contribué à la construction de les deux axes (alpha.var=“contrib”) ou celle qui sont le plus prise en compte par les deux axes (alpha.var=“cos2”), soit encore de sélectionner celles qui ont un cos2 supérieur à un seuil (select.var = list(cos2 = 0.75) ) ou les 3 qui contribuent le plus (select.var = list(contrib = 3))): fviz_pca_var(pca, axes = c(1, 2), alpha.var=&quot;contrib&quot;)+theme_minimal() fviz_pca_var(pca, axes = c(1, 2), alpha.var=&quot;cos2&quot;)+ theme_minimal() fviz_pca_var(pca, axes = c(1, 2), select.var = list(cos2 = 0.75))+ theme_minimal() fviz_pca_var(pca, axes = c(1, 2), select.var = list(contrib = 3))+ theme_minimal() Il est peut aussi être pertinent d’utiliser l’argument repel=T pour utiliser le package repel pour les étiquettes et éviter qu’elle ne se chevauches: fviz_pca_var(pca, axes = c(1, 2), repel=T )+ theme_minimal() 2.7.2 Contributions de chaque variables La contribution de chaque variable à la construction de chacune de dimension: pca$var$contrib ## Dim.1 Dim.2 Dim.3 ## ARTHAUD 11.9038406 1.231246e+00 0.183052896 ## POUTOU 10.2770061 7.451467e-04 11.017687192 ## ROUSSEL 9.2175433 5.126181e-01 5.648180142 ## MELENCHON 11.4639072 9.528856e+00 7.305250657 ## JADOT 4.2401420 9.339684e+00 27.736460944 ## HIDALGO 0.2176015 1.550210e+01 24.259841866 ## LASSALLE 0.1224650 1.915665e+01 12.764245064 ## MACRON 18.3188531 7.720568e-02 1.743810408 ## PECRESSE 16.2650617 3.390215e+00 0.006233513 ## ZEMMOUR 12.9781893 4.546192e+00 0.630488306 ## DUPONTAIGNAN 0.7816796 1.522150e+01 8.037025154 ## MLEPEN 4.2137107 2.149298e+01 0.667723858 ## Dim.4 Dim.5 Dim.6 ## ARTHAUD 19.83392132 9.58930308 17.567987257 ## POUTOU 1.00132471 10.65438844 59.780728472 ## ROUSSEL 70.59679126 4.22707266 4.124807104 ## MELENCHON 0.07249975 1.74743980 0.014316739 ## JADOT 0.27708309 0.06781672 1.163893177 ## HIDALGO 5.75125091 0.40760167 0.401878970 ## LASSALLE 0.17673574 3.73427942 8.815479728 ## MACRON 0.10195308 0.10505823 0.456531526 ## PECRESSE 0.33161753 3.77877609 0.075911050 ## ZEMMOUR 0.73142974 7.15362007 5.980334606 ## DUPONTAIGNAN 0.33684917 58.20260409 1.613713804 ## MLEPEN 0.78854370 0.33203971 0.004417566 ## Dim.7 Dim.8 Dim.9 Dim.10 ## ARTHAUD 38.26088046 1.1309610 1.056017e-02 0.05892929 ## POUTOU 0.15638066 6.6239329 2.541885e-04 0.20145614 ## ROUSSEL 4.06960429 0.3508970 3.770484e-01 0.66905864 ## MELENCHON 0.39703930 0.2583124 1.399810e+01 0.10020919 ## JADOT 0.03138149 0.8675954 1.901157e-03 42.90604037 ## HIDALGO 0.03705574 24.7230208 1.538862e-02 28.27347030 ## LASSALLE 22.58997116 2.6517620 2.830917e+01 0.56791122 ## MACRON 0.63089386 13.4317208 7.660100e+00 0.85049882 ## PECRESSE 3.29004564 5.1707694 4.637305e-02 19.45128139 ## ZEMMOUR 11.39892589 29.2405834 1.664638e+01 5.28825575 ## DUPONTAIGNAN 13.21034744 0.3933712 1.722571e+00 0.24640866 ## MLEPEN 5.92747407 15.1570737 3.121215e+01 1.38648022 ## Dim.11 Dim.12 ## ARTHAUD 2.190093e-01 0.01030897 ## POUTOU 2.716054e-01 0.01449063 ## ROUSSEL 3.535471e-02 0.17102445 ## MELENCHON 1.796628e+00 53.31743705 ## JADOT 1.196316e+01 1.40484268 ## HIDALGO 3.091431e-01 0.10164445 ## LASSALLE 1.023255e+00 0.08807739 ## MACRON 3.446917e+01 22.15420622 ## PECRESSE 4.541062e+01 2.78309654 ## ZEMMOUR 2.453912e+00 2.95168558 ## DUPONTAIGNAN 8.411265e-06 0.23392274 ## MLEPEN 2.048139e+00 16.76926331 On peut également afficher de façon graphique avec fviz_contrib: fviz_contrib(pca, choice = &quot;var&quot;, axes = 1, top = 10) fviz_contrib(pca, choice = &quot;var&quot;, axes = 2, top = 10) 2.7.3 Valeurs propres et % de la variance expliquée Pour afficher les valeurs propres (eigen value) et le pourcentage de variance expliquée, on peut simplement afficher le tableau compris dans l’objet pca pca$eig ## eigenvalue percentage of variance ## comp 1 4.824670e+00 4.020558e+01 ## comp 2 3.047316e+00 2.539430e+01 ## comp 3 1.586296e+00 1.321913e+01 ## comp 4 5.558034e-01 4.631695e+00 ## comp 5 5.268516e-01 4.390430e+00 ## comp 6 4.179668e-01 3.483056e+00 ## comp 7 3.866235e-01 3.221863e+00 ## comp 8 2.524500e-01 2.103750e+00 ## comp 9 1.932934e-01 1.610779e+00 ## comp 10 1.188874e-01 9.907285e-01 ## comp 11 8.984151e-02 7.486793e-01 ## comp 12 1.045041e-15 8.708673e-15 ## cumulative percentage of variance ## comp 1 40.20558 ## comp 2 65.59988 ## comp 3 78.81902 ## comp 4 83.45071 ## comp 5 87.84114 ## comp 6 91.32420 ## comp 7 94.54606 ## comp 8 96.64981 ## comp 9 98.26059 ## comp 10 99.25132 ## comp 11 100.00000 ## comp 12 100.00000 On peut faire un graphique de la variance expliquées: fviz_screeplot(pca, ncp=10) Ces informations sont utiles pour nous aider à décider combien de composante retenir. Généralement, on s’arrange avec ces règles non-strictes: - eigenvalue &gt; 1 - variance cumulée &gt; 70% - gain faible d’une composante supplémentaire (ajouter une dimension supplémentaire ne permet pas de gagner beaucoup en variance expliquée) 2.7.4 Carte de scores Chaque individus est reprojeter sur les nouvelles dimensions. Leurs scores sont stocké dans l’objet pca: head(pca$ind$coord) ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 ## 1 3.067555 -1.7114152 0.08150264 -0.185735850 -0.41247012 ## 2 2.511828 -2.7064534 0.70245008 -0.193923389 -0.70891924 ## 3 2.031107 -3.0966114 1.07311078 -0.404287195 -0.18691460 ## 4 2.201343 -2.4124210 1.18680228 -0.001301227 -0.20907736 ## 5 1.991101 -2.2577563 1.02941720 -0.014047196 -0.02204558 ## 6 3.955340 -0.8826657 -0.20675170 0.106337247 0.01854102 ## Dim.6 Dim.7 Dim.8 Dim.9 Dim.10 ## 1 -0.1498315 -0.31386218 0.51897558 -0.4392267 0.16799431 ## 2 -0.4567220 -0.53505432 0.36256495 -0.6205415 -0.13143940 ## 3 -0.5270521 -0.32566793 0.04083631 -0.7457277 -0.17322620 ## 4 0.2881203 -0.18494092 0.55795434 -0.3835738 0.18158093 ## 5 -0.1995434 -0.07027549 0.24404548 -0.1594431 0.07828135 ## 6 -0.1812643 0.07415939 -0.23014907 -0.3167121 0.16569411 ## Dim.11 Dim.12 ## 1 -0.45550607 -2.122990e-09 ## 2 -0.64944655 -2.768542e-09 ## 3 -0.51307636 -6.765777e-09 ## 4 -0.51829968 -1.631543e-09 ## 5 -0.03678451 -1.845831e-09 ## 6 -0.35174294 -2.268829e-09 On peut souhaiter les visualiser dans un graphique à deux dimensions: fviz_pca_ind(pca) Forcément cela à du sens que si on a peu d’individus. Si les individus sont des entités géographiques, on peut souhaiter en faire des cartes. On peut récupérer les score de chaque individus en collant avec bind_rows les coordonnées avec le set de données initiale. On doit utiliser ici bind_rows plutôt qu’une jointure (left_join, right_join) parce que l’agorythme de FactoMineR ne conserve pas d’ID lors de la réalisation de l’ACP mais il conserve bien l’ordre des lignes mise en entrée: data_pca&lt;-bind_cols(data, pca$ind$coord) On réalise une boucle pour visualiser les 3 premières dimensions en faisant d’abord la jointure entre nos résultats et l’objet commune: communes_pca&lt;-communes %&gt;% inner_join(data_pca, by=c(&quot;insee&quot;=&quot;codecommune&quot;)) for (i in 1:3){ # mf_export(x = communes_pca , # filename =paste0(&quot;TP09/carte_acp_dim&quot;,i,&quot;.png&quot; ), # width = 900) mf_map(communes_pca , var= paste0(&quot;Dim.&quot;,i), type=&quot;choro&quot;, pal= &quot;Viridis&quot;, lwd=0.001) mf_map(x = departements_Paris, col = NA, border = &quot;gray25&quot;, lwd = 1, add=T) # dev.off() } rm(i,communes_pca) 2.8 Ajouter des variables supplémentaires Pour aider à l’interprétation des axes, il peut être utile d’importer des nouvelles variables qui ne seront pas utilisées dans l’ACP mais qu’on pourra néanmoins utiliser comme repère. 2.8.1 Importation des données 2.8.1.1 Population On importe les données de population: pop&lt;-read_excel(&quot;data/France/population_insee_2021.xlsx&quot;, sheet=&quot;Communes&quot;) %&gt;% mutate(codecommune=paste0(`Code département`, `Code commune`)) %&gt;% filter(`Code région`==&quot;11&quot;) # on garde que la France métropolitaine On joint avec le fichier spatial des communes et calcul la densité de population communes_pop&lt;- communes %&gt;% left_join(pop, by=c(&quot;insee&quot;=&quot;codecommune&quot;)) %&gt;% mutate(densite_ha=`Population totale`/surface) %&gt;% select(insee, densite_ha) On peut réaliser une carte de la densité de population # mf_export(x = communes_pop , # filename =paste0(&quot;TP09/carte_densite.png&quot; ), # width = 900) mf_map(communes_pop , var= &quot;densite_ha&quot;, type=&quot;choro&quot;, pal= &quot;Viridis&quot;, lwd=0.001) mf_map(x = departements_Paris, col = NA, border = &quot;red&quot;, lwd = 1, add=T) #dev.off() On enlève transforme l’objet en dataframe et on enlève la colonne geom: communes_pop_df&lt;-communes_pop %&gt;% as.data.frame() %&gt;% select(-geom) On réalise la jointure avec les données de base: data&lt;-data %&gt;% left_join(communes_pop_df, by=c(&quot;codecommune&quot;=&quot;insee&quot;)) rm(pop,communes_pop, communes_pop_df) 2.8.1.2 Revenus On importe les données de revenus, on conserve les champ qui nous intéresse et on transforme le champ revenu médian en “numeric”: revenus&lt;-read_excel(&quot;data/France/FILO2021_DEC_COM.xlsx&quot;, sheet=2) %&gt;% mutate(revenus=as.numeric(revenu_median)) %&gt;% select(CODGEO, revenus) On joint avec le fichier spatial des communes et les revenus: communes_revenus&lt;- communes %&gt;% left_join(revenus, by=c(&quot;insee&quot;=&quot;CODGEO&quot;)) On peut réaliser une carte de revenu: # mf_export(x = communes_revenus , # filename =paste0(&quot;TP09/carte_revenus.png&quot; ), # width = 900) mf_map(communes_revenus , var= &quot;revenus&quot;, type=&quot;choro&quot;, pal= &quot;Viridis&quot;, lwd=0.001) mf_map(x = departements_Paris, col = NA, border = &quot;red&quot;, lwd = 1, add=T) #dev.off() rm(communes_revenus) On réalise la jointure avec les données de base: data&lt;-data %&gt;% left_join(revenus, by=c(&quot;codecommune&quot;=&quot;CODGEO&quot;)) rm(revenus) 2.8.2 ACP avec les variables supplémentaires pca&lt;-PCA(data[,20:33], quanti.sup = 13:14, row.w =data$exprimes, graph=F, ncp = NULL) ## Warning in PCA(data[, 20:33], quanti.sup = 13:14, row.w = ## data$exprimes, : Missing values are imputed by the mean of ## the variable: you should use the imputePCA function of the ## missMDA package cercle_1_2&lt;-fviz_pca_var(pca, axes = c(1, 2))+ theme_minimal() cercle_1_2 #ggsave(cercle_1_2,filename=&quot;TP09/cercle_correlation_1_2_revenu_pop.png&quot;, width=7, height=7, bg=&quot;white&quot;) rm(cercle_1_2) cercle_1_3&lt;-fviz_pca_var(pca, axes = c(1, 3))+ theme_minimal() cercle_1_3 #ggsave(cercle_1_3,filename=&quot;TP09/cercle_correlation_1_3_revenu_pop.png&quot;, width=7, height=7, bg=&quot;white&quot;) rm(cercle_1_3) On observe des fortes corrélations entre la première dimension qui dépeind une opposition gauche-droite et la variable de revenus, les revenus élevés étant associés aux communes où le vote de droite est important. La seconde dimension est quant à elle fortement corrélée à la densité de population et permet d’interprétée cette dernière comme l’opposition centre urbain-périphérie 2.9 Aller plus loin Il est possible de faire des ACP sur des données d’enquêtes soit en utilisant les poids comme présenté précédement soit grâce au package :svyprcomp (https://r-survey.r-forge.r-project.org/survey/html/svyprcomp.html) On peut vouloir réaliser des légères rotations des axes pour améliorer la lecture des axes: https://stats.stackexchange.com/questions/59213/how-to-compute-varimax-rotated-principal-components-in-r https://dimension.usherbrooke.ca/pages/87 https://sites.google.com/site/rgraphiques/4--stat/machine-learning-biostatistiques-analyse-de-donn%C3%A9es/analyse-en-composantes-principales/la-rotation-varimax 2.10 Ressources utilisées: http://www.sthda.com/english/wiki/wiki.php?id_contents=7851 http://factominer.free.fr/index_fr.html "],["classification.html", "Chapter 3 Classification 3.1 Objectifs 3.2 Préparation 3.3 ACP 3.4 Classification ascendente hiérarchique 3.5 Visualisation des résultats 3.6 Ressources utilisées", " Chapter 3 Classification 3.1 Objectifs Ce tp a pour objectif de réaliser une classification ascendante hiérarchique des secteurs statistiques de Bruxelles en utilisant des variables de la structure de la propriété des logements mis en location. 3.2 Préparation De façon maintenant habituelle: library(tidyverse) # Manipulation des données library(sf) # Manipulation des données spatiales library(mapsf) # Réalisation de carte library(readxl) # Importation de données Excel library(kableExtra) # Réalisation de tableau Et deux packages développés pour réaliser des ACP et visualiser les résultats: library(FactoMineR) library(factoextra) Pour ce TP, on utilisera également le packages JLutils: #library(devtools) #devtools::install_github(&quot;larmarange/JLutils&quot;) library(JLutils) Pour cette classification, on utiliser des indicateurs construits à partir des données du cadastre (1 janvier 2015) utilisée dans ma thèse (https://difusion.ulb.ac.be/vufind/Record/ULB-DIPOT:oai:dipot.ulb.ac.be:2013/359086/Holdings) Les variables utilisées sont: cd_sector = code secteur statistique NB_LOG = nombre de logements total NB_LOGLOUE_TOT = nombre de logements loués total NB_LOCDOM = nombre de logements loués à domicile prop_occup = proportion de propriétaires occupants prop_LOCDOM = proportion de logements loués à domicile prop_LOC_HBX = proportion de logements loués par des propriétaires habitants en dehors de la région de Bruxelles dist_200 = proportion de logements loués par des propriétaires domiciliés à moins de 200 m cinq_plus = proportion de logements loués possédés par des propriétaires possédants 5 logements ou plus à Bruxelles vingt_plus = proportion de logements loués possédés par des propriétaires possédants 20 logements ou plus à Bruxelles log_sociaux = proportion des logements sociaux parmi les logements loués entrep_sprl_sa = proportion des logements possédés par des entreprises privées parmi les logements loués cadastre &lt;- read_delim( &quot;data/cadastre_2015_secteurs.csv&quot;, delim=&quot;;&quot;) %&gt;% filter(NB_LOGLOUE_TOT&gt;=200) ## Rows: 724 Columns: 12 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): cd_sector ## dbl (11): NB_LOG, NB_LOGLOUE_TOT, NB_LOCDOM, prop_occup, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 3.3 ACP Pour réaliser la classification, on va d’abord réaliser une classification. Cela nous permet d’éliminer le bruit. pca&lt;-PCA(cadastre[,6:12], row.w =cadastre$NB_LOGLOUE_TOT,ncp = NULL) On se demande combien de composante garder: les 3 premières représentes 80% de la variance. fviz_screeplot(pca, ncp=10) pca$eig On peut joindre les socres des ACP aux données cadastre&lt;-bind_cols(cadastre,pca$ind$coord) 3.4 Classification ascendente hiérarchique 3.4.1 Calcul de la classification On extrait les scores des trois premières composantes qu’on va utiliser dans la classification: score&lt;-pca$ind$coord[,1:3] On calcul une matrice de distance euclidienne: dist &lt;- dist(score, method = &quot;euclidean&quot;) On réalise la classification avec la méthode de Ward: hcpc &lt;- hclust(dist, method = &quot;ward.D&quot; ) 3.4.2 Choix du nombre de classe On peut réaliser le dendogramme : plot(hcpc, cex = 0.6, hang = -1) Ainsi que le graphique d’inertie: inertie &lt;- sort(hcpc$height, decreasing = TRUE) plot(inertie[1:20], type = &quot;s&quot;, xlab = &quot;Nombre de classes&quot;, ylab = &quot;Inertie&quot;) Il apparaît ici que le gain pour ajouter une classe supplémentaire diminue fortement après la 5ème classe On définit donc le paramètre k=5: #png(filename = &quot;TP10/dendogramme.png&quot;) plot(hcpc, cex = 0.6, hang = -1) rect.hclust(hcpc , k = 5, border = 2:6) #dev.off() Il existe une fonction de JLutils qui permet de claculer le nombre de classes “optimale”. best.cutree(hcpc) ## [1] 4 3.5 Visualisation des résultats 3.5.1 Extraire les groupes On peut alors associer chaque individus (ici les secteurs statistiques) à chaque un groupe: hcpc_indiv &lt;- cutree(hcpc, k = 5) On peut alors rapatrier les résultats sur les données de bases: cadastre&lt;-cadastre %&gt;% mutate(clust= as.factor(hcpc_indiv)) 3.5.2 Un graphique selon les groupes On peut réaliser un graphique où on place chaque individus (ici les secteurs statistiques) sur les deux dimensions construites lors de l’ACP et on colorie les points selon la classe: ggplot(data=cadastre, aes(x=Dim.1, y=Dim.2, color=clust)) + geom_point()+ theme_minimal() 3.5.3 Une carte des groupes On importe les secteurs statistiques et garde que les secteurs bruxellois: secteurs_stats&lt;- st_read (&quot;data/sh_statbel_statistical_sectors_31370_20230101.gpkg&quot;) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) ## Reading layer `secteurs_stats2023&#39; from data source ## `C:\\Users\\hugop\\Nextcloud\\git\\manuel_geo_quanti\\data\\sh_statbel_statistical_sectors_31370_20230101.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 19795 features and 31 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 21991.63 ymin: 21162.16 xmax: 295167.1 ymax: 244027.2 ## Projected CRS: BD72 / Belgian Lambert 72 On construit un objet spatial communes pour la cartographie: communes&lt;-secteurs_stats %&gt;% group_by(tx_munty_descr_fr) %&gt;% summarise(geom=st_union(geom)) On joint les groupes aux secteurs statistiques: secteurs_stats_cad&lt;- secteurs_stats %&gt;% left_join(cadastre, by=&quot;cd_sector&quot;) On réalise la carte: # On peut aussi définir une palette de couleur pour facilité la lecture pal &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;purple&quot;, &quot;green&quot;, &quot;orange&quot;) # mf_export(x = secteurs_stats_cad , # filename =paste0(&quot;TP10_typo/carte_typo.png&quot; ), # width = 900) mf_map(x = secteurs_stats, col = NA, border = &quot;gray25&quot;, lwd = 0.3) mf_map(secteurs_stats_cad , var= c(&quot;NB_LOGLOUE_TOT&quot;,&quot;clust&quot;), type=&quot;prop_typo&quot;, inches = 0.08, pal= pal, lwd=0.3, add=T) ## 225 &#39;NA&#39; values are not plotted on the map. mf_map(x = communes, col = NA, border = &quot;gray25&quot;, lwd = 1, add=T) #dev.off() 3.5.4 Un tableau de statistique pour chaque groupes On réalise des statistiques (ici moyenne pondérée par le nombre de logements loués) par groupes: tab&lt;-cadastre %&gt;% group_by(clust) %&gt;% summarise( log_sociaux= weighted.mean(log_sociaux,NB_LOGLOUE_TOT ), entrep_sprl_sa= weighted.mean(entrep_sprl_sa,NB_LOGLOUE_TOT ), prop_LOCDOM= weighted.mean(prop_LOCDOM,NB_LOGLOUE_TOT ), prop_LOC_HBX = weighted.mean(prop_LOC_HBX,NB_LOGLOUE_TOT ), dist_200 = weighted.mean(dist_200,NB_LOGLOUE_TOT ), cinq_plus= weighted.mean(cinq_plus,NB_LOGLOUE_TOT ), vingt_plus= weighted.mean(vingt_plus,NB_LOGLOUE_TOT ) ) # On ajoute une ligne pour la moyenne: moyenne_total&lt;-cadastre %&gt;% summarise( log_sociaux= weighted.mean(log_sociaux,NB_LOGLOUE_TOT ), entrep_sprl_sa= weighted.mean(entrep_sprl_sa,NB_LOGLOUE_TOT ), prop_LOCDOM= weighted.mean(prop_LOCDOM,NB_LOGLOUE_TOT ), prop_LOC_HBX = weighted.mean(prop_LOC_HBX,NB_LOGLOUE_TOT ), dist_200 = weighted.mean(dist_200,NB_LOGLOUE_TOT ), cinq_plus= weighted.mean(cinq_plus,NB_LOGLOUE_TOT ), vingt_plus= weighted.mean(vingt_plus,NB_LOGLOUE_TOT ) ) # On place le résultat dans la ligne donc le numéro est équivalent au nombre de ligne dans le tableau plus une (nrow()+1) pour les colonnes de 2 à 8: tab[nrow(tab)+1 , 2:8]&lt;-moyenne_total #On multiplie toutes les cellules par 100 et on arrondi avec un 1 chiffre après la virgule: tab[,2:8]&lt;-round(100*tab[,2:8],1) On peut alors donner des noms aux groupes tab$clust&lt;-as.character(tab$clust) tab&lt;- tab %&gt;% mutate(clust=case_when ( clust ==1~&quot;1. Propriétaires à proximité&quot;, clust ==2~&quot;2. Grands propriétaires et entreprises&quot;, clust ==3~&quot;3. Type moyen&quot;, clust ==4~&quot;4. Propriétaires non-Bruxellois&quot;, clust ==5~&quot;5. Logements sociaux&quot;, is.na(clust)~&quot;Moyenne&quot;)) kable(tab) clust log_sociaux entrep_sprl_sa prop_LOCDOM prop_LOC_HBX dist_200 cinq_plus vingt_plus 1. Propriétaires à proximité 10.2 6.7 27.6 19.8 67.3 29.6 5.3 2. Grands propriétaires et entreprises 3.4 26.0 5.8 26.4 26.4 47.3 20.1 3. Type moyen 2.8 13.1 14.0 32.1 52.9 32.9 8.2 4. Propriétaires non-Bruxellois 3.1 10.4 9.6 39.2 32.7 19.7 3.4 5. Logements sociaux 78.0 1.9 2.4 7.1 7.8 13.7 3.3 Moyenne 10.4 11.1 15.0 28.1 46.8 29.1 7.1 Le package JLutils permet également de colorer le dendogramme: # library(JLutils) A2Rplot(hcpc, k =5, boxes = T, show.labels = F,col.up = &quot;gray50&quot;, col.down = pal, main=&quot;Dendrogramme - 5 classes&quot;) 3.5.5 Comparaison avec une autre variable On importe l’indice synthétique de difficulté: ind_synth &lt;-read_delim(&quot;data/indice_synthetique.csv&quot;, delim=&quot;;&quot;) ## Rows: 7752 Columns: 2 ## ── Column specification ───────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): Secteur statistique ## dbl (1): Indice synthétique de difficulté 2010 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. indice_sec&lt;-secteurs_stats %&gt;% left_join(ind_synth, by=c(&quot;cd_sector&quot;=&quot;Secteur statistique&quot;)) %&gt;% filter(tx_rgn_descr_fr==&quot;Région de Bruxelles-Capitale&quot;) mf_map(indice_sec, var=&quot;Indice synthétique de difficulté 2010&quot;, type=&quot;choro&quot;, pal= &quot;Viridis&quot;, nbreaks=5) On joint l’indice synthétique de difficulté avec les données sur base du secteur statistique: cadastre&lt;- cadastre %&gt;% left_join(ind_synth,by= c(&quot;cd_sector&quot;= &quot;Secteur statistique&quot;)) On réalise un graphique avec des boxplot: ggplot(data=cadastre,aes(x=clust, y=`Indice synthétique de difficulté 2010`, weight=NB_LOGLOUE_TOT, fill=clust)) + geom_boxplot(fill=pal) + labs(title =&quot;Relation classification et indice synthétique de difficulté&quot;, x= &quot; &quot;, y= &quot;Indice synth. de diff. 2010&quot;)+ theme_minimal() On peut en conclure qu’il y a un lien entre la division sociale de l’espace et la structure de la propriété des logements loués. Les secteurs du groupe 1 sont plus en difficulté que les autres secteurs, surtout les 3 et 4. Le secteurs centraux (2) ont des indices intermédiaires et certains secteurs de logements sociaux (5) sont très mixtes d’où leur forte dispersion. 3.6 Ressources utilisées https://larmarange.github.io/analyse-R/classification-ascendante-hierarchique.html http://factominer.free.fr/factomethods/classification-hierarchique-sur-composantes-principales.html http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/117-hcpc-hierarchical-clustering-on-principal-components-essentials/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
